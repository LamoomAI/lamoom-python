{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Lamoom\n",
    "\n",
    "Welcome to Lamoom - your platform for building, testing, and optimizing LLM prompts at scale!\n",
    "\n",
    "This notebook will guide you through:\n",
    "1. Setting up your environment\n",
    "2. Creating a simple prompt\n",
    "3. Testing prompt quality with real-world data\n",
    "4. Analyzing and visualizing results\n",
    "\n",
    "Let's begin by installing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "! pip install lamoom\n",
    "! pip install openai\n",
    "! pip install pandas\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "For this tutorial, we'll import the necessary libraries and set up our environment. \n",
    "Go to to https://portal.lamoom.com/settings/api_tokens and generate for you a new token, set as LAMOOM_API_TOKEN; \n",
    "\n",
    "> **Note:** You'll need to have an `.env` file with your API keys or set them as environment variables. Lamoom supports multiple LLM providers including OpenAI, Claude, Azure OpenAI, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries and load environment variables\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating Your First Prompt\n",
    "\n",
    "Lamoom uses a simple but powerful prompt creation interface. A Prompt object serves as a container for your prompt template, which can include dynamic variables enclosed in `{variable_name}` syntax.\n",
    "\n",
    "Below, we'll create a medical question-answering prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lamoom import Prompt\n",
    "\n",
    "# Create a prompt with unique ID\n",
    "agent = Prompt(\"answer_on_medical_question\")\n",
    "\n",
    "# Add instructions with a dynamic variable for the question\n",
    "agent.add(\"\"\"Answer on medical question of the user:\n",
    "{question}\n",
    "          \n",
    "Please first think out loud before answering.\n",
    "\"\"\", role=\"system\")\n",
    "\n",
    "# The {question} placeholder will be filled with actual questions at runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting up the Lamoom Client\n",
    "\n",
    "Now let's initialize the Lamoom client with your API keys. We'll also set up the testing pipeline that will help us evaluate our prompt quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lamoom import Lamoom\n",
    "from lamoom.response_parsers.response_parser import get_json_from_response\n",
    "\n",
    "# Get API keys from environment variables\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the Lamoom client\n",
    "client = Lamoom(openai_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our test dataset\n",
    "data_df = pd.read_csv('./test_data/medical_questions_answers.csv')\n",
    "\n",
    "# Take a peek at the data\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each test case\n",
    "results = []\n",
    "for index, row in data_df.iterrows():\n",
    "    print(f\"Processing test case {index+1}/{len(data_df)}\")\n",
    "    \n",
    "    # Extract question and ideal answer from our dataset\n",
    "    question = row['question']\n",
    "    ideal_answer = row['answer_provided_by_human']\n",
    "    \n",
    "    try:\n",
    "        # Prepare the context (variables that will be injected into our prompt)\n",
    "        context = {\n",
    "            'question': question,\n",
    "            'ideal_answer': ideal_answer,  # This won't be shown to the LLM but used for evaluation\n",
    "        }\n",
    "        \n",
    "        # Call the LLM with our prompt, context, and model\n",
    "        # The test_data parameter enables automatic test creation in Lamoom\n",
    "        response = client.call(\n",
    "            agent.id,             # The prompt ID we created earlier\n",
    "            context,              # Variables to inject into the prompt\n",
    "            'openai/o3-mini',      # The model to use\n",
    "            test_data={\n",
    "                'ideal_answer': ideal_answer  # Reference answer for creating tests in Lamoom https://lamoom.com\n",
    "            }\n",
    "        )\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'ideal_answer': ideal_answer,\n",
    "            'response': response.content,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test case {index}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Test Results Online\n",
    "\n",
    "Lamoom provides built-in visualization tools to help you understand your prompt's performance. Let's view a summary of our test results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Test Results Locally\n",
    "\n",
    "Lamoom provides built-in visualization tools to help you understand your prompt's performance. Let's view a summary of our test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lamoom_cicd\n",
    "\n",
    "from lamoom_cicd import TestLLMResponsePipe\n",
    "# Initialize testing pipeline\n",
    "lamoom_pipe = TestLLMResponsePipe(openai_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each test case\n",
    "cicd_pipeline_results = []\n",
    "for row in results:\n",
    "    try:\n",
    "        # Compare the LLM's response to our ideal answer\n",
    "        test_result = lamoom_pipe.compare(row['ideal_answer'], row['response'], optional_params={'question': row['question']})\n",
    "        # Store the results for later analysis\n",
    "        lamoom_pipe.accumulated_results.append(test_result)\n",
    "        cicd_pipeline_results.append(test_result)  \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test case {index}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate visualization of test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamoom_pipe.visualize_test_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Dive into Detailed Results\n",
    "\n",
    "The chart above gives us a high-level view of how our prompt performed across test cases. Now let's look at the detailed evaluation for each test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results for the first 3 test cases\n",
    "for i, result in enumerate(cicd_pipeline_results[:3]):\n",
    "    print(f\"\\n==== Test Case {i+1} ====\")\n",
    "    print(f\"Score: {result.questions[0].does_match_ideal_answer}\")\n",
    "    print(f\"\\nQuestion: {result.questions[0].test_question}\")\n",
    "    print(f\"\\nIdeal Answer: {result.questions[0].ideal_answer}\")\n",
    "    print(f\"\\nModel Response: {result.questions[0].llm_answer}\")\n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Continuous Improvement with Lamoom\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. Created a prompt with dynamic content\n",
    "2. Tested it against real-world examples\n",
    "3. Evaluated and visualized its performance\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "With Lamoom, you can take your prompt engineering to the next level:\n",
    "\n",
    "**üöÄ Prompt Management**\n",
    "- Create, version, and maintain a library of prompts\n",
    "- Organize prompts by use case, model, or performance metrics\n",
    "\n",
    "**üß™ Testing & CI/CD**\n",
    "- Set up automated testing pipelines\n",
    "- Get alerts when prompt performance degrades\n",
    "- Compare model performance across different providers\n",
    "\n",
    "**üìä Analytics**\n",
    "- Track cost, latency, and usage metrics\n",
    "- Identify opportunities for optimization\n",
    "- Monitor prompt performance in production\n",
    "\n",
    "**üîÑ Iterative Development**\n",
    "- Use the web interface to refine prompts collaboratively\n",
    "- Run A/B tests on prompt variations\n",
    "- Deploy without code changes\n",
    "\n",
    "### Visit Us\n",
    "\n",
    "- üåê [Lamoom Cloud](https://portal.lamoom.com) - Manage all your prompts in one place\n",
    "- üìö [Documentation](https://docs.lamoom.com) - Learn all about Lamoom's features\n",
    "- üßë‚Äçüíª [GitHub](https://github.com/orgs/lamoom-team) - Check out our open source repositories\n",
    "\n",
    "---\n",
    "\n",
    "*Thank you for exploring Lamoom! If you have any questions, reach out to us at support@lamoom.com*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
